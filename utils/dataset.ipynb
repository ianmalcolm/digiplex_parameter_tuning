{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feat3Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, adj_path, da_path, transform=None):\n",
    "        \"\"\" adj_path(string): path to the adjacency matrix\n",
    "            da_path(string): path to da results, each file in the path has four columns:\n",
    "                A, probability of feasibility, avg and std of objective energy\n",
    "            transform(callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "\n",
    "        da_files = os.listdir(da_path)\n",
    "        adj_files = os.listdir(adj_path)\n",
    "\n",
    "        self.df = None\n",
    "        self.transform = transform\n",
    "        \n",
    "        for entry in adj_files:\n",
    "            if not entry+'.csv' in da_files:\n",
    "                continue\n",
    "\n",
    "            adj_data = adj_file_handler(os.path.join(adj_path, entry))\n",
    "            da_data = da_pd_file_handler(os.path.join(da_path, entry+'.csv'))\n",
    "    \n",
    "            da_data['graph'] = entry\n",
    "            da_data['feat3'] = None\n",
    "            da_data['feat3'] = da_data['feat3'].apply(lambda x: adj_data)\n",
    "\n",
    "            if self.df is None:\n",
    "                self.df = da_data\n",
    "            else:\n",
    "                self.df = self.df.append(da_data, ignore_index=True)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        elif isinstance(idx, (int, np.integer)):\n",
    "            idx = [idx]\n",
    "        if any([i not in range(len(self)) for i in idx]):\n",
    "            raise IndexError\n",
    "\n",
    "        _input = self.df.loc[idx, ['feat3','A']].apply(lambda x: np.hstack([x['feat3'],[x['A']]]), axis=1)\n",
    "        _target = self.df.loc[idx, ['PF', 'Eavg', 'Estd']]\n",
    "        _graph = self.df.loc[idx, 'graph']\n",
    "        _dmin = self.df.loc[idx, 'dists'].apply(lambda x: np.nan if len(x)==0 else x.min())\n",
    "        \n",
    "        batch = {'input':np.vstack(_input.values), \n",
    "                 'target':np.vstack(_target.values), \n",
    "                 'graph':np.vstack(_graph.values), \n",
    "                 'dmin':np.vstack(_dmin.values),}\n",
    "        \n",
    "        if self.transform:\n",
    "            batch = self.transform(batch)\n",
    "        \n",
    "        if batch['input'].shape[0]==1:\n",
    "            batch['input'] = batch['input'][0]\n",
    "            batch['target'] = batch['target'][0]\n",
    "            batch['graph'] = batch['graph'][0]\n",
    "            batch['dmin'] = batch['dmin'][0]\n",
    "            \n",
    "        return batch\n",
    "    \n",
    "    def bestSet(self, pf):\n",
    "        groups = self.df.groupby('graph')\n",
    "        for name, group in groups:\n",
    "            idx = group[group['PF']>=pf]['Eavg'].idxmin(axis=0)\n",
    "            yield self[idx]\n",
    "\n",
    "    def getGroup(self):\n",
    "        return self.df['graph'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feat300Dataset(Feat3Dataset):\n",
    "    \n",
    "    def __init__(self, feat_path, adj_path, da_path, transform=None):\n",
    "        \"\"\" feat_path(string): path to the features, each file in the path has 300 features \n",
    "                for each edge\n",
    "            adj_path(string): path to the adjacency matrix\n",
    "            da_path(string): path to da results, each file in the path has four columns:\n",
    "                A, probability of feasibility, avg and std of objective energy\n",
    "            transform(callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        feat_files = os.listdir(feat_path)\n",
    "        da_files = os.listdir(da_path)\n",
    "        adj_files = os.listdir(adj_path)\n",
    "\n",
    "        self.df = None\n",
    "        self.transform = transform\n",
    "        \n",
    "        for entry in adj_files:\n",
    "            if not entry+'.npy' in feat_files:\n",
    "                continue\n",
    "            if not entry+'.csv' in da_files:\n",
    "                continue\n",
    "\n",
    "            adj_data = adj_file_handler(os.path.join(adj_path, entry))\n",
    "            feat_data = feat_file_handler(os.path.join(feat_path, entry+'.npy'))\n",
    "            da_data = da_pd_file_handler(os.path.join(da_path, entry+'.csv'))\n",
    "    \n",
    "            da_data['graph'] = entry\n",
    "            da_data['feat3'] = None\n",
    "            da_data['feat3'] = da_data['feat3'].apply(lambda x: adj_data)\n",
    "            da_data['feat300'] = None\n",
    "            da_data['feat300'] = da_data['feat3'].apply(lambda x: feat_data)\n",
    "\n",
    "            if self.df is None:\n",
    "                self.df = da_data\n",
    "            else:\n",
    "                self.df = self.df.append(da_data, ignore_index=True)\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        elif isinstance(idx, (int, np.integer)):\n",
    "            idx = [idx]\n",
    "        if any([i not in range(len(self)) for i in idx]):\n",
    "            raise IndexError\n",
    "\n",
    "        _input = self.df.loc[idx, ['feat300','feat3','A']].apply(lambda x: np.hstack([x['feat300'],x['feat3'],[x['A']]]), axis=1)\n",
    "        _target = self.df.loc[idx, ['PF', 'Eavg', 'Estd']]\n",
    "        _graph = self.df.loc[idx, 'graph']\n",
    "        _dmin = self.df.loc[idx, 'dists'].apply(lambda x: np.nan if len(x)==0 else x.min())\n",
    "        \n",
    "        batch = {'input':np.vstack(_input.values), \n",
    "                 'target':np.vstack(_target.values), \n",
    "                 'graph':np.vstack(_graph.values), \n",
    "                 'dmin':np.vstack(_dmin.values),}\n",
    "        \n",
    "        if self.transform:\n",
    "            batch = self.transform(batch)\n",
    "        \n",
    "        if batch['input'].shape[0]==1:\n",
    "            batch['input'] = batch['input'][0]\n",
    "            batch['target'] = batch['target'][0]\n",
    "            batch['graph'] = batch['graph'][0]\n",
    "            batch['dmin'] = batch['dmin'][0]\n",
    "            \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_file_handler(adj_filename):\n",
    "    adj = np.loadtxt(adj_filename)\n",
    "    elements = adj[np.tril_indices(adj.shape[0])]\n",
    "    avg = np.average(elements)\n",
    "    std = np.std(elements)\n",
    "    dim = adj.shape[0]\n",
    "    \n",
    "    return np.asarray([avg, std, dim])\n",
    "\n",
    "def feat_file_handler(feat_filename):\n",
    "    feat = np.load(feat_filename).mean(axis=(0,1))\n",
    "    return feat\n",
    "\n",
    "def da_file_handler(da_filename):\n",
    "    da = np.loadtxt(da_filename, delimiter=',')\n",
    "    return da\n",
    "\n",
    "def da_csv_file_handler(da_filename):\n",
    "    da_data = pd.read_csv(da_filename)\n",
    "    da_data.columns = [col.strip() for col in da_data.columns]\n",
    "    set_trace()\n",
    "    data = da_data[['A', 'da_distance', 'objective_energy']].to_numpy()\n",
    "    ret = []\n",
    "    a_set = np.unique(data[:,0])\n",
    "    for a in a_set:\n",
    "        subset = data[data[:,0]==a, :]\n",
    "        pf = 1-(subset[:,1]==-1).sum()/subset.shape[0]\n",
    "        avg = subset[:,2].mean()\n",
    "        std = subset[:,2].std()\n",
    "        row = [a, pf, avg, std]\n",
    "        ret.append(row)\n",
    "    return np.asarray(ret)\n",
    "\n",
    "def da_pd_file_handler(da_filename):\n",
    "    data = pd.read_csv(da_filename)\n",
    "    data['feasibility'] = data['da_distance']!=-1\n",
    "    groups = data.groupby('A')\n",
    "    eavg = groups['objective_energy'].mean()\n",
    "    estd = groups['objective_energy'].std()\n",
    "    pf = groups['feasibility'].apply(lambda x: x.sum()/x.shape[0])\n",
    "    dists = groups['da_distance'].apply(lambda x: x[x>0].values)\n",
    "    return pd.concat([pf, eavg, estd, dists], axis=1, keys=['PF', 'Eavg', 'Estd', 'dists']).reset_index()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatTransformer:\n",
    "    \"\"\"Transform a batch of data to mean of zero and std of one. PF is left untouched.\n",
    "\n",
    "    Args:\n",
    "        scalerx (StandardScaler): for input feature normalization, feature + A\n",
    "        scalery (StandardScaler): for target feature normalization, PF, Eavg and Estd and Dmin\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scalerx, scalery):\n",
    "        self.scalerx = scalerx\n",
    "        self.scalery = scalery\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "\n",
    "        sample['input'] = self.scalerx.transform(sample['input'])\n",
    "        \n",
    "        _target = self.scalery.transform(sample['target'])\n",
    "        _target[:, 0] = sample['target'][:, 0]\n",
    "        sample['target'] = _target\n",
    "        \n",
    "        _dummyY = np.zeros_like(sample['target'])\n",
    "        _dummyY[:, 1] = sample['dmin'][:, 0]\n",
    "        sample['dmin'] = self.scalery.transform(_dummyY)[:, 1]\n",
    "        \n",
    "        return sample\n",
    "\n",
    "    def inverse_transform_Y(self, Y):\n",
    "        Y_orig = self.scalery.inverse_transform(Y)\n",
    "        Y_orig[:,0] = Y[:,0]\n",
    "        return Y_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': array([ 1.53057419,  1.43540825,  0.7874925 , -1.90561279]), 'target': array([ 0.        , -1.847292  , -0.01290751]), 'graph': array(['JHFrZFk_27'], dtype='<U10'), 'dmin': array([nan])}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Demo the usage\n",
    "    \n",
    "    adj_path = '../dataset/round3/adj/'\n",
    "    da_path = '../dataset/round3/da/'\n",
    "    dataset = Feat3Dataset(adj_path=adj_path, da_path=da_path)\n",
    "    X = np.asarray([d['input'] for d in dataset])\n",
    "    Y = np.asarray([d['target'] for d in dataset])\n",
    "    scalerx = StandardScaler().fit(X)\n",
    "    scalery = StandardScaler().fit(Y)\n",
    "    transform = FeatTransformer(scalerx, scalery)\n",
    "    dataset = Feat3Dataset(adj_path=adj_path, da_path=da_path, transform=transform)\n",
    "\n",
    "    for inst in dataset:\n",
    "        print(inst)\n",
    "        break\n",
    "\n",
    "\n",
    "#     print()\n",
    "\n",
    "#     feat_path = '../dataset/round3/feat300/'\n",
    "#     adj_path = '../dataset/round3/adj/'\n",
    "#     da_path = '../dataset/round3/da/'\n",
    "#     dataset = Feat300Dataset(feat_path=feat_path, adj_path=adj_path, da_path=da_path)\n",
    "#     X = np.asarray([d['input'] for d in dataset])\n",
    "#     Y = np.asarray([d['target'] for d in dataset])\n",
    "#     scalerx = StandardScaler().fit(X)\n",
    "#     scalery = StandardScaler().fit(Y)\n",
    "#     transform = FeatTransformer(scalerx, scalery)\n",
    "#     dataset = Feat300Dataset(feat_path=feat_path, adj_path=adj_path, da_path=da_path, transform=transform)\n",
    "\n",
    "#     for inst in dataset.bestSet(0.5):\n",
    "#         print(inst)\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
