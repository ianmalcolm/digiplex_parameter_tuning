{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dummyAPI import api as smu_api\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--extend'], dest='extend', nargs=0, const=True, default=False, type=None, choices=None, help='Extend dataset', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Sampling As for a folder of graphs')\n",
    "parser.add_argument('--graph_path', type=str, required=True)\n",
    "parser.add_argument('--da_path', type=str, default='log')\n",
    "parser.add_argument('--a_per_graph', type=int, default=100)\n",
    "parser.add_argument('--extend', action='store_true', help='Extend dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global args\n",
    "    \n",
    "    cmd = '--graph_path round6/adj'\n",
    "    cmd += ' --da_path round6/da'\n",
    "    args = parser.parse_args(cmd.split(' '))\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # Check the save_dir exists or not\n",
    "    if not os.path.exists(args.da_path):\n",
    "        os.mkdir(args.da_path)\n",
    "        \n",
    "    if args.extend:\n",
    "        # The coverage of A is not good enough for digiplex hyper-parameter tuning\n",
    "        # The goal of the snippet of code is to extend the dataset a little\n",
    "        \n",
    "        for entry in os.listdir(args.graph_path):\n",
    "\n",
    "            daFilename = os.path.join(args.da_path, entry)+'.csv'\n",
    "            gFilename = os.path.join(args.graph_path, entry)\n",
    "\n",
    "            if not os.path.exists(daFilename):\n",
    "                print('Skip {} because no da file is found'.format(gFilename), flush=True)\n",
    "                continue\n",
    "\n",
    "            a_list = generate_a(daFilename)\n",
    "            \n",
    "            if len(a_list)==0:\n",
    "                print('Skip {} because it has already been extended'.format(gFilename), flush=True)\n",
    "                continue\n",
    "\n",
    "            func = lambda A: evaluate(gFilename, A, daDir=args.da_path)[0]\n",
    "            \n",
    "            print('Extending {}'.format(gFilename), end='', flush=True)\n",
    "            for a in generate_a(daFilename):\n",
    "                func(a)\n",
    "                print('.', end='', flush=True)\n",
    "            print('done', flush=True)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        for entry in os.listdir(args.graph_path):\n",
    "            daFilename = os.path.join(args.da_path, entry)+'.csv'\n",
    "            gFilename = os.path.join(args.graph_path, entry)\n",
    "            if os.path.exists(daFilename):\n",
    "                print('Skip {}'.format(gFilename))\n",
    "                continue\n",
    "            evaluateGraph(gFilename, numSample=args.a_per_graph, daDir=args.da_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API related stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateGraphAPair(gFilename, A, timeout=1200):\n",
    "    \"\"\" Given graph in np.loadtxt compatible format, an A\n",
    "        returns the da data including \n",
    "         A  iterationnumber  da_distance  objective_energy\n",
    "         objective_energy_unperturbed  column_constraint_energy\n",
    "         row_constraint_energy  \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    waitTotal = 0\n",
    "    waitInterval = 5\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            status_code, reason, text = smu_api.post(gFilename, A)\n",
    "            break\n",
    "        except:\n",
    "            print('Submit job ({},{}) failed, retry'.format(gFilename, A))\n",
    "            time.sleep(waitInterval)\n",
    "\n",
    "\n",
    "    ret = 'init'\n",
    "\n",
    "    while type(ret) == str:\n",
    "        time.sleep(waitInterval)\n",
    "#         ret = smu_api.get(text)      \n",
    "        while True:\n",
    "            try:\n",
    "                ret = smu_api.get(text)\n",
    "                break\n",
    "            except:\n",
    "                print('Retrieve job ({},{}) failed, retry'.format(gFilename, A))\n",
    "                time.sleep(waitInterval)\n",
    "        \n",
    "        waitTotal += waitInterval\n",
    "        if waitTotal>timeout:\n",
    "            return None\n",
    "\n",
    "    return ret\n",
    "\n",
    "def getStatistics(da_data):\n",
    "    \"\"\" Given da_data in pandas dataframe format\n",
    "        returns the probability of feasibility\n",
    "        mean of objective_energy\n",
    "        std of objective_energy\n",
    "        \n",
    "    \"\"\"\n",
    "    pf = (da_data[['da_distance']]!=-1).to_numpy().sum() / da_data.shape[0]\n",
    "    avg_obj = da_data[['objective_energy']].mean().to_numpy()[0]\n",
    "    std_obj = da_data[['objective_energy']].std().to_numpy()[0]\n",
    "    return pf, avg_obj, std_obj\n",
    "\n",
    "def saveProgress(entry, da_data, daDir='log/'):\n",
    "    if not os.path.exists(daDir):\n",
    "        os.mkdir(daDir)\n",
    "\n",
    "    da_filename = os.path.join(daDir, entry)+'.csv'\n",
    "    \n",
    "    if not os.path.isfile(da_filename):\n",
    "        da_data.to_csv(da_filename, header=True, index=False, mode='a+')\n",
    "    else:\n",
    "        da_data.to_csv(da_filename, header=False, index=False, mode='a+')\n",
    "\n",
    "def evaluate(gFilename, A, daDir='log/'):\n",
    "    \n",
    "    da_data = None\n",
    "    RETRY = 3\n",
    "    nTry = 0\n",
    "\n",
    "    while da_data is None and nTry < RETRY: \n",
    "\n",
    "        # evaluate the given graph A pair\n",
    "        da_data = evaluateGraphAPair(gFilename, A)\n",
    "        \n",
    "        nTry += 1\n",
    "    \n",
    "    if da_data is None:\n",
    "        print('Failed on graph, A: {}, {}'.format(gFilename, A))\n",
    "        return None\n",
    "\n",
    "    # calculate the statistics of the da_data\n",
    "    pf, avg_obj, std_obj = getStatistics(da_data)\n",
    "    \n",
    "    # save statistics for future use\n",
    "    baseName = os.path.basename(gFilename)\n",
    "    saveProgress(baseName, da_data, daDir=daDir)\n",
    "    \n",
    "    return pf, avg_obj, std_obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Range of A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guessA(gFilename):\n",
    "    \"\"\" Initial guess about A\n",
    "        A good start could be the Average of edge weight\n",
    "    \"\"\"\n",
    "    graph = np.loadtxt(gFilename)\n",
    "    return graph.max()\n",
    "\n",
    "def findACoarseRange(A, func):\n",
    "    \"\"\" Given a random guess of A, find Amin and Amax exponentially\n",
    "        Amin is the bigest A that leads to PF=0\n",
    "        Amax is the smallest A that leads to PF=1\n",
    "        It's OK if the Amin and Amax we find is far away from ground truth\n",
    "        We will carry further search to approach the ground truth\n",
    "    \"\"\"\n",
    "    p = func(A)\n",
    "    if p<=0:\n",
    "        # when the guess leads to p==0\n",
    "        Amin = A\n",
    "        Amax = Amin * 2\n",
    "        while p<1:\n",
    "            p=func(Amax)\n",
    "            if p<=0:\n",
    "                Amin = Amax\n",
    "                Amax = Amin * 2\n",
    "            elif p>0 and p<1:\n",
    "                Amax += Amax - Amin\n",
    "    elif p>0 and p<1:\n",
    "        # when the guess leads to p in (0,1)\n",
    "        Amin = A\n",
    "        Amax = A\n",
    "        while p>0:\n",
    "            p=func(Amin)\n",
    "            if p>0:\n",
    "                Amin /= 2\n",
    "        while p<1:\n",
    "            p=func(Amax)\n",
    "            if p<1:\n",
    "                Amax += Amax-Amin\n",
    "    else:\n",
    "        # when the guess leads to p==1\n",
    "        Amax = A\n",
    "        Amin = Amax / 2\n",
    "        while p>0:\n",
    "            p=func(Amin)\n",
    "            if p>=1:\n",
    "                Amax = Amin\n",
    "                Amin = Amax / 2\n",
    "            elif p>0 and p<1:\n",
    "                Amin /= 2\n",
    "                \n",
    "    return Amin, Amax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAFineRange(Amin, Amax, func):\n",
    "    \"\"\" Given coarse range of A, find fine range of A using iterative grid search\n",
    "    \"\"\"\n",
    "    betterAmin, betterAmax = Amin, Amax\n",
    "    flagUpdated = True\n",
    "    while flagUpdated:\n",
    "        flagUpdated = False\n",
    "        As = np.linspace(betterAmin, betterAmax, num=5)[1:-1]\n",
    "        pfs = np.asarray([func(a) for a in As])\n",
    "\n",
    "        indices = np.where(pfs<=0)[0]\n",
    "        if len(indices)>0:\n",
    "            betterAmin = As[indices[-1]]\n",
    "#             print('Amin updated to {}'.format(betterAmin))\n",
    "            flagUpdated = True\n",
    "        indices = np.where(pfs>=1)[0]\n",
    "        if len(indices)>0:\n",
    "            betterAmax = As[indices[0]]\n",
    "#             print('Amax updated to {}'.format(betterAmax))\n",
    "            flagUpdated = True\n",
    "    \n",
    "    return betterAmin, betterAmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniformSample(Amin, Amax, func, num=100, report_interval=10):\n",
    "    \"\"\" Sample As uniformly within given range\n",
    "    \"\"\"\n",
    "    assert Amax>Amin\n",
    "    assert Amax>0 and Amin>0\n",
    "    As = np.linspace(Amin, Amax, num=num+2)[1:-1]\n",
    "\n",
    "    for i, a in enumerate(As):\n",
    "        func(a)\n",
    "        if i % report_interval == report_interval-1:\n",
    "            print('.', end='')\n",
    "\n",
    "    print('done')\n",
    "    \n",
    "def gaussianSample(avg, std, func, num=100, report_interval=10):\n",
    "    \"\"\" Sample gaussianly with given mean and std\n",
    "    \"\"\"\n",
    "    \n",
    "    As = np.random.normal(avg, std, num)\n",
    "    \n",
    "    for i, a in enumerate(As):\n",
    "        if a<0:\n",
    "            func(-a)\n",
    "        else:\n",
    "            func(a)\n",
    "        if i % report_interval == report_interval-1:\n",
    "            print('.', end='')\n",
    "\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_a(da_filename, enlarge=5, num_bin=20, random_seed=None):\n",
    "    if random_seed:\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "    da_data = pd.read_csv(da_filename)\n",
    "    \n",
    "    da_pf = da_data.groupby('A').agg(pf=('da_distance', lambda x: (x>0).values.sum()/x.size))\n",
    "    da_pf.sort_index(inplace=True)\n",
    "    a_pf_0 = da_pf[da_pf['pf']>0].head(1).index.values[0]\n",
    "    a_pf_1 = da_pf[da_pf['pf']<1].tail(1).index.values[0]\n",
    "    a_pf_range = a_pf_1-a_pf_0\n",
    "    \n",
    "    half_side = (enlarge-1)/2\n",
    "\n",
    "    a_min = max(1, a_pf_0 - a_pf_range * half_side)\n",
    "    a_max = a_pf_1 + a_pf_range * half_side\n",
    "    bins = np.linspace(a_min, a_max, num=num_bin)\n",
    "\n",
    "    hist, bins = np.histogram(da_pf.index.values, bins=bins)\n",
    "\n",
    "    random_factor = np.random.rand((hist==0).sum()) * (a_max-a_min) / (num_bin-1)\n",
    "    return bins[:-1][hist==0] + random_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateGraph(gFilename, numSample=100, daDir='log/'):\n",
    "    func = lambda A: evaluate(gFilename, A, daDir=daDir)[0]\n",
    "    print('Evaluating As for graph {}'.format(gFilename))\n",
    "    initA = guessA(gFilename)\n",
    "    print('\\tInitial guess of A is {}'.format(initA))\n",
    "    Amin, Amax = findACoarseRange(initA, func)\n",
    "    print('\\tFind coarse range of A: [{},{}]'.format(Amin, Amax))\n",
    "    Amin, Amax = findAFineRange(Amin, Amax, func)\n",
    "    print('\\tFind fine range of A: [{},{}]'.format(Amin, Amax))\n",
    "    print('\\tSweeping A ranging [{},{}]'.format(Amin, Amax), end='')\n",
    "    uniformSample(Amin, Amax, func, num=numSample//5*4)\n",
    "    print('\\tGaussian sampling A with (avg, std): ({},{})'.format((Amax+Amin)/2, (Amax-Amin)/2), end='')\n",
    "    gaussianSample((Amax+Amin)/2, (Amax-Amin)/2, func, num=numSample//5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
